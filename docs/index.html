import openai
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# ----------- A. Simulated Data Sources -----------
crm = {
    "cust-1234": {"name": "Jane Doe", "orders": ["Order#1245 - Delivered 2 days ago"]}
}
knowledge_base = [
    {"id": 1, "text": "Return within 30 days, free shipping", "tags": ["policy", "return"]},
    {"id": 2, "text": "How do I return a defective item?", "tags": ["faq", "return"]}
]
chat_logs = [
    {"customer": "Jane Doe", "msg": "I want to return my item", "sentiment": "neutral"}
]

# ----------- B. Ingestion & Processing -----------
def chunk_document(doc, max_tokens=30):
    # Dummy chunker: splits by sentences for demo
    return [s.strip() for s in doc.split('.') if s.strip()]

kb_chunks = []
for kb in knowledge_base:
    for chunk in chunk_document(kb["text"]):
        kb_chunks.append({"text": chunk, "tags": kb["tags"], "id": kb["id"]})

# Enrich with metadata
for chunk in kb_chunks:
    chunk["customer_id"] = "cust-1234"
    chunk["region"] = "US"

# ----------- C. Vector DB Build (FAISS) -----------
embedder = SentenceTransformer('all-MiniLM-L6-v2')
corpus = [chunk["text"] for chunk in kb_chunks]
corpus_embeddings = embedder.encode(corpus)
index = faiss.IndexFlatL2(corpus_embeddings.shape[1])
index.add(np.array(corpus_embeddings))

# ----------- D. Context Engineering Layer -----------
def classify_intent(query):
    # Dummy intent classifier
    if "return" in query.lower():
        return "return_request", 0.93
    return "other", 0.80

def search_context(query, top_k=2):
    query_emb = embedder.encode([query])
    D, I = index.search(np.array(query_emb), top_k)
    return [kb_chunks[i] for i in I[0]]

def mask_pii(context):
    # Mask name for demo
    for c in context:
        c["text"] = c["text"].replace("Jane Doe", "[REDACTED]")
    return context

def package_envelope(context, customer_id, confidence):
    customer = crm[customer_id]
    envelope = {
        "customer": customer["name"],
        "recent_orders": customer["orders"],
        "policy": next((c["text"] for c in context if "policy" in c["tags"]), ""),
        "related_faq": next((c["text"] for c in context if "faq" in c["tags"]), ""),
        "confidence": confidence
    }
    return envelope

# ----------- E. LLM Orchestration Layer -----------
def call_llm(envelope):
    openai.api_key = "sk-..."  # Put your OpenAI key here
    prompt = f"""Customer: {envelope['customer']}
Recent Orders: {envelope['recent_orders']}
Policy: {envelope['policy']}
FAQ: {envelope['related_faq']}
Query confidence: {envelope['confidence']}
Respond to customer question: "How do I return a defective item?"
"""
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role":"system","content":"You are a helpful support agent."},
                  {"role":"user","content": prompt}]
    )
    return response['choices'][0]['message']['content']

# ----------- F. Feedback Loop -----------
def capture_feedback(response, useful=True):
    print("Feedback captured:", {"response": response, "useful": useful})

# ----------- G. Observability & Trust -----------
def log_observability(envelope, response):
    print("Context Envelope:", envelope)
    print("LLM Response:", response)

# ----------- MAIN FLOW -----------
if __name__ == "__main__":
    query = "How do I return a defective item?"
    customer_id = "cust-1234"
    intent, confidence = classify_intent(query)
    context_chunks = search_context(query)
    context_chunks = mask_pii(context_chunks)
    envelope = package_envelope(context_chunks, customer_id, confidence)
    llm_response = call_llm(envelope)
    log_observability(envelope, llm_response)
    capture_feedback(llm_response, useful=True)
